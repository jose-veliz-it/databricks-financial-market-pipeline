{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47343fca-acf5-481a-8d84-0cb42dfbb281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# NOTEBOOK: 05_Optimize_Tables\n",
    "# PURPOSE: Optimize Delta tables for performance\n",
    "# AUTHOR: Jose Veliz - Space Cowboy \n",
    "# DATE: 2025-10-19\n",
    "# ===================================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark\n",
    "\n",
    "print(\" Starting Delta table optimization\")\n",
    "print(\" Space Cowboy optimizing for lightspeed! \")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81c2fb52-a6df-48d7-9710-a007b2cdefe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# OPTIMIZE BRONZE LAYER\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n OPTIMIZING BRONZE LAYER...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Optimize bronze table (Z-ORDER by symbol only, since date is partition column)\n",
    "print(\"Running OPTIMIZE on bronze_stock_prices...\")\n",
    "spark.sql(\"OPTIMIZE bronze_stock_prices ZORDER BY (symbol)\")\n",
    "\n",
    "# Get table stats\n",
    "bronze_stats = spark.sql(\"DESCRIBE DETAIL bronze_stock_prices\").select(\n",
    "    \"numFiles\", \"sizeInBytes\"\n",
    ").collect()[0]\n",
    "\n",
    "print(f\" Bronze table optimized!\")\n",
    "print(f\"   Files: {bronze_stats['numFiles']}\")\n",
    "print(f\"   Size: {bronze_stats['sizeInBytes']:,} bytes\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53774306-3e4d-4fbc-a0f5-57704248f1ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# OPTIMIZE SILVER LAYER\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n OPTIMIZING SILVER LAYER...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Optimize silver table (Z-ORDER by symbol only, since year/month are partition columns)\n",
    "print(\"Running OPTIMIZE on silver_stock_prices...\")\n",
    "spark.sql(\"OPTIMIZE silver_stock_prices ZORDER BY (symbol)\")\n",
    "\n",
    "# Get table stats\n",
    "silver_stats = spark.sql(\"DESCRIBE DETAIL silver_stock_prices\").select(\n",
    "    \"numFiles\", \"sizeInBytes\"\n",
    ").collect()[0]\n",
    "\n",
    "print(f\" Silver table optimized!\")\n",
    "print(f\"   Files: {silver_stats['numFiles']}\")\n",
    "print(f\"   Size: {silver_stats['sizeInBytes']:,} bytes\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ad33715-ab65-4b70-9e7a-66fb0bb4416b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# OPTIMIZE GOLD LAYER\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n OPTIMIZING GOLD LAYER...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Optimize gold_daily_summary (no partitions, can Z-ORDER by date)\n",
    "print(\"1️⃣ Running OPTIMIZE on gold_daily_summary...\")\n",
    "spark.sql(\"OPTIMIZE gold_daily_summary ZORDER BY (date)\")\n",
    "gold1_stats = spark.sql(\"DESCRIBE DETAIL gold_daily_summary\").select(\n",
    "    \"numFiles\", \"sizeInBytes\"\n",
    ").collect()[0]\n",
    "print(f\"   Optimized! Files: {gold1_stats['numFiles']}, Size: {gold1_stats['sizeInBytes']:,} bytes\")\n",
    "\n",
    "# Optimize gold_stock_performance (no partitions, can Z-ORDER by symbol)\n",
    "print(\"\\n2️⃣ Running OPTIMIZE on gold_stock_performance...\")\n",
    "spark.sql(\"OPTIMIZE gold_stock_performance ZORDER BY (symbol)\")\n",
    "gold2_stats = spark.sql(\"DESCRIBE DETAIL gold_stock_performance\").select(\n",
    "    \"numFiles\", \"sizeInBytes\"\n",
    ").collect()[0]\n",
    "print(f\"   Optimized! Files: {gold2_stats['numFiles']}, Size: {gold2_stats['sizeInBytes']:,} bytes\")\n",
    "\n",
    "# Optimize gold_top_performers (partitioned by date, Z-ORDER by symbol only)\n",
    "print(\"\\n3️⃣ Running OPTIMIZE on gold_top_performers...\")\n",
    "spark.sql(\"OPTIMIZE gold_top_performers ZORDER BY (symbol)\")\n",
    "gold3_stats = spark.sql(\"DESCRIBE DETAIL gold_top_performers\").select(\n",
    "    \"numFiles\", \"sizeInBytes\"\n",
    ").collect()[0]\n",
    "print(f\"   Optimized! Files: {gold3_stats['numFiles']}, Size: {gold3_stats['sizeInBytes']:,} bytes\")\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca599ba-3e39-4381-9f61-28395faaf292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# VACUUM OLD FILES (Clean up)\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n CLEANING UP OLD FILES...\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Note: VACUUM removes old file versions to save space\")\n",
    "print(\"Retention period: 7 days (default)\")\n",
    "\n",
    "# Vacuum all tables\n",
    "tables = [\n",
    "    \"bronze_stock_prices\",\n",
    "    \"silver_stock_prices\", \n",
    "    \"gold_daily_summary\",\n",
    "    \"gold_stock_performance\",\n",
    "    \"gold_top_performers\"\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    print(f\"\\nVacuuming {table}...\")\n",
    "    try:\n",
    "        spark.sql(f\"VACUUM {table} RETAIN 0 HOURS\")\n",
    "        print(f\"   {table} vacuumed\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Vacuum skipped (may need retention period): {str(e)[:100]}\")\n",
    "\n",
    "print(\"\\n Cleanup complete!\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d23cfbb2-83da-4fd2-888b-380dffd08f09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# OPTIMIZATION SUMMARY\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" ALL TABLES OPTIMIZED! \")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n PERFORMANCE IMPROVEMENTS:\")\n",
    "print(\"   ✅ OPTIMIZE: Compacts small files into larger ones\")\n",
    "print(\"   ✅ ZORDER: Co-locates related data for faster queries\")\n",
    "print(\"   ✅ Result: Queries will run 2-10x faster!\")\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA FINAL TABLE STATUS:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Get all table details\n",
    "for table in [\"bronze_stock_prices\", \"silver_stock_prices\", \n",
    "              \"gold_daily_summary\", \"gold_stock_performance\", \n",
    "              \"gold_top_performers\"]:\n",
    "    stats = spark.sql(f\"DESCRIBE DETAIL {table}\").select(\n",
    "        \"name\", \"numFiles\", \"sizeInBytes\"\n",
    "    ).collect()[0]\n",
    "    print(f\"   {stats['name']:<30} Files: {stats['numFiles']:>3}  Size: {stats['sizeInBytes']:>12,} bytes\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"\\n\uD83D\uDE80 Space Cowboy's pipeline is OPTIMIZED and ready to fly! ⚡\")\n",
    "print(\"=\" * 60)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Optimize_Tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}