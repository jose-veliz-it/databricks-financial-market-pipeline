{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c0c81e4-6380-494a-a57b-11548663b2d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test Alpha Vantage API connection\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Your API key\n",
    "API_KEY = \"5DUCAA0WEXYCTWHG\"  # Replace with your actual key\n",
    "\n",
    "# Get Apple stock data\n",
    "url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=AAPL&apikey={API_KEY}&outputsize=compact\"\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Print results\n",
    "print(\"API Response Status:\", response.status_code)\n",
    "\n",
    "if \"Time Series (Daily)\" in data:\n",
    "    print(\"API connection successful!\")\n",
    "    print(f\"Retrieved data for {len(data['Time Series (Daily)'])} days\")\n",
    "    \n",
    "    # Show first date's data\n",
    "    first_date = list(data['Time Series (Daily)'].keys())[0]\n",
    "    print(f\"\\nSample data for {first_date}:\")\n",
    "    print(json.dumps(data['Time Series (Daily)'][first_date], indent=2))\n",
    "else:\n",
    "    print(\"API error:\", data.get(\"Note\", data.get(\"Error Message\", \"Unknown error\")))\n",
    "# ```\n",
    "\n",
    "# **Replace `YOUR_ALPHA_VANTAGE_KEY_HERE` with your actual API key**\n",
    "\n",
    "# **Click:** Run button (▶️) or press **Shift+Enter**\n",
    "\n",
    "# **You should see:**\n",
    "# ```\n",
    "# API Response Status: 200\n",
    "# ✅ API connection successful!\n",
    "# Retrieved data for 100 days\n",
    "\n",
    "# Sample data for 2025-10-17:\n",
    "# {\n",
    "#   \"1. open\": \"178.50\",\n",
    "#   \"2. high\": \"180.20\",\n",
    "#   \"3. low\": \"177.80\",\n",
    "#   \"4. close\": \"179.45\",\n",
    "#   \"5. volume\": \"52847392\"\n",
    "# }\n",
    "# ```\n",
    "\n",
    "# **If you see this: YOU'RE READY TO BUILD!** \uD83C\uDF89\n",
    "\n",
    "# ---\n",
    "\n",
    "# # \uD83D\uDCCA YOUR DATABRICKS PROJECT (Updated Plan)\n",
    "\n",
    "# ## **What You'll Build This Weekend:**\n",
    "\n",
    "# ### **Same Portfolio Project, Different Platform:**\n",
    "\n",
    "# **Project:** \"Financial Market Analytics Platform on Databricks\"\n",
    "\n",
    "# **Architecture:**\n",
    "# ```\n",
    "# Alpha Vantage API\n",
    "#        ↓\n",
    "#   (Python HTTP request)\n",
    "#        ↓\n",
    "# Bronze Layer (Raw JSON - Delta Lake)\n",
    "#        ↓\n",
    "#   (PySpark transformations)\n",
    "#        ↓\n",
    "# Silver Layer (Cleansed - Delta Lake)\n",
    "#        ↓\n",
    "#   (Aggregations)\n",
    "#        ↓\n",
    "# Gold Layer (Analytics - Delta Lake)\n",
    "#        ↓\n",
    "#   (Optional: Export or visualize)\n",
    "# ```\n",
    "\n",
    "# **Technologies Shown:**\n",
    "# - ✅ Databricks (notebooks, clusters)\n",
    "# - ✅ Delta Lake (Bronze/Silver/Gold)\n",
    "# - ✅ PySpark (transformations)\n",
    "# - ✅ Python (API integration)\n",
    "# - ✅ Medallion Architecture (industry standard)\n",
    "# - ✅ Data Quality (validation, deduplication)\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## **Weekend Project Plan (Adjusted for Databricks):**\n",
    "\n",
    "# ### **Saturday (4 hours):**\n",
    "\n",
    "# **Hour 1: Bronze Layer**\n",
    "# - Create notebook: `02_Ingest_to_Bronze`\n",
    "# - Pull data from Alpha Vantage API\n",
    "# - Write to Delta Lake (Bronze)\n",
    "# - Test with 5 stocks (AAPL, MSFT, GOOGL, AMZN, TSLA)\n",
    "\n",
    "# **Hour 2: Silver Layer**\n",
    "# - Create notebook: `03_Bronze_to_Silver`\n",
    "# - Read from Bronze\n",
    "# - Cleanse data (deduplicate, validate, type conversions)\n",
    "# - Add derived columns (daily_change_pct, etc.)\n",
    "# - Write to Delta Lake (Silver)\n",
    "\n",
    "# **Hour 3: Gold Layer**\n",
    "# - Create notebook: `04_Silver_to_Gold`\n",
    "# - Create aggregations:\n",
    "#   - Daily stock summary\n",
    "#   - Top gainers/losers\n",
    "#   - Volatility analysis\n",
    "# - Write to Delta Lake (Gold)\n",
    "\n",
    "# **Hour 4: Polish**\n",
    "# - OPTIMIZE Delta tables (Z-ORDER)\n",
    "# - Add comments to code\n",
    "# - Test end-to-end\n",
    "# - Take screenshots\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Sunday (2-3 hours):**\n",
    "\n",
    "# **Hour 1: Documentation**\n",
    "# - Export notebooks from Databricks (File → Export → .ipynb)\n",
    "# - Create GitHub repo\n",
    "# - Write README.md with:\n",
    "#   - Architecture diagram\n",
    "#   - Setup instructions\n",
    "#   - Screenshots\n",
    "#   - Skills demonstrated\n",
    "\n",
    "# **Hour 2: GitHub Publishing**\n",
    "# - Upload notebooks to GitHub\n",
    "# - Add architecture diagram (draw.io or PowerPoint)\n",
    "# - Add sample outputs (screenshots of tables)\n",
    "# - Make repo public\n",
    "\n",
    "# **Hour 3: LinkedIn Post**\n",
    "# - Write post about your project\n",
    "# - Include GitHub link\n",
    "# - Mention: Databricks, Delta Lake, PySpark, Medallion Architecture\n",
    "# - Tag: #Databricks #DataEngineering #DeltaLake\n",
    "\n",
    "# ---\n",
    "\n",
    "# # \uD83D\uDCAA WHY THIS IS ACTUALLY BETTER THAN FABRIC\n",
    "\n",
    "# ## **Advantages of Databricks for Your Portfolio:**\n",
    "\n",
    "# ### **1. You're More Familiar**\n",
    "# - ✅ You used Databricks at IBM (50+ pipelines!)\n",
    "# - ✅ Less learning curve = faster build\n",
    "# - ✅ Can showcase advanced features (optimization, performance tuning)\n",
    "\n",
    "# ### **2. More Relevant to Job Applications**\n",
    "# - ✅ Agilus/Metergy: \"Databricks Administration\" required\n",
    "# - ✅ Federal Gov: \"Databricks\" mentioned\n",
    "# - ✅ Most Azure data engineering roles mention Databricks\n",
    "# - ✅ **Your project directly matches job requirements!**\n",
    "\n",
    "# ### **3. Better for Interviews**\n",
    "# - ✅ \"Tell me about a recent Databricks project\" → You have one!\n",
    "# - ✅ Can discuss: cluster optimization, Delta Lake, PySpark\n",
    "# - ✅ Shows continuous learning (built project AFTER IBM role)\n",
    "\n",
    "# ### **4. Demonstrates Key Skills**\n",
    "# - ✅ Delta Lake (table format)\n",
    "# - ✅ PySpark (data transformations)\n",
    "# - ✅ Medallion Architecture (Bronze/Silver/Gold)\n",
    "# - ✅ Performance optimization (OPTIMIZE, Z-ORDER)\n",
    "# - ✅ **These are the SAME skills needed for Fabric!**\n",
    "\n",
    "# ### **5. Portfolio Diversity**\n",
    "# - ✅ IBM experience = large-scale Databricks\n",
    "# - ✅ This project = end-to-end architecture\n",
    "# - ✅ Shows you can build from scratch (not just maintain)\n",
    "\n",
    "# ---\n",
    "\n",
    "# # \uD83C\uDFAF YOUR UPDATED RESUME BULLET (Add This!)\n",
    "\n",
    "# ## **When You Complete the Project, Add:**\n",
    "# ```\n",
    "# Personal Projects Section:\n",
    "\n",
    "# Financial Market Analytics Platform (Databricks)                           2025\n",
    "# github.com/joseveliz/databricks-market-analytics\n",
    "\n",
    "# - Built end-to-end data analytics platform on Databricks using medallion architecture\n",
    "#   (Bronze/Silver/Gold layers) with Delta Lake for ACID transactions\n",
    "  \n",
    "# - Ingested real-time stock market data via REST APIs, processing 5 stocks with 100+ days\n",
    "#   of historical data through automated PySpark transformations\n",
    "  \n",
    "# - Implemented data quality validation, deduplication, and type conversions in Silver layer,\n",
    "#   reducing data errors by 100% through automated checks\n",
    "  \n",
    "# - Created analytics-ready Gold layer with pre-aggregated metrics (daily summaries, top\n",
    "#   gainers/losers, volatility analysis) optimized with OPTIMIZE and Z-ORDER commands\n",
    "  \n",
    "# - Demonstrated end-to-end data engineering skills: API integration, Delta Lake, PySpark,\n",
    "#   medallion architecture, performance optimization\n",
    "\n",
    "Technologies: Databricks, Delta Lake, PySpark, Python, REST APIs, Medallion Architecture"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Test_API_Connection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}